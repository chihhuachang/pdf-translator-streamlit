import streamlit as st
import google.generativeai as genai
import os
import sys
import io
import time # å¼•å…¥ time æ¨¡çµ„ç”¨æ–¼æ·»åŠ å»¶é²

# Import libraries for file reading AND writing docx
import docx
from docx import Document
import PyPDF2
from PyPDF2.errors import PdfReadError

# --- å¸¸æ•¸å®šç¾© ---
# å®šç¾©æ¯å€‹æ–‡æœ¬å¡Šçš„æœ€å¤§å­—ç¬¦æ•¸ (å¯æ ¹æ“šéœ€è¦èª¿æ•´ï¼Œ Gemeni Pro ç´„æœ‰ 30k token é™åˆ¶ï¼ŒFlash è¼ƒå°‘ï¼Œä¿å®ˆä¸€é»)
# æ³¨æ„ï¼šé€™åªæ˜¯å€‹å¤§æ¦‚å€¼ï¼Œå¯¦éš› token æ•¸èˆ‡å­—ç¬¦æ•¸ä¸å®Œå…¨å°æ‡‰
MAX_CHARS_PER_CHUNK = 2500
# æ¯æ¬¡ API å‘¼å«ä¹‹é–“çš„å»¶é²ï¼ˆç§’ï¼‰ï¼Œé¿å…è§¸åŠé€Ÿç‡é™åˆ¶ (å…è²»ç‰ˆé€šå¸¸æœ‰æ¯åˆ†é˜è«‹æ±‚é™åˆ¶)
API_CALL_DELAY = 2

# --- èªªæ˜æ–‡ä»¶ ---
# åŠŸèƒ½ï¼šä¸Šå‚³è‹±æ–‡æ–‡ä»¶(txt, docx, pdf)ï¼Œè®€å–å…§å®¹ï¼Œå°‡é•·æ–‡æœ¬ **åˆ†å¡Š(Chunking)**ï¼Œ
#       ä½¿ç”¨å›ºå®šçš„è©³ç´°æç¤ºè©å’Œæµå¼å‚³è¼¸é€å¡Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼Œ
#       åˆä½µçµæœå¾Œæä¾› .docx æª”æ¡ˆä¸‹è¼‰ã€‚
# (å…¶é¤˜èªªæ˜èˆ‡ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ)
#
# å¦‚ä½•åŸ·è¡Œï¼š
# 1. å®‰è£å‡½å¼åº«: pip install streamlit google-generativeai python-docx PyPDF2
# 2. å°‡æ­¤ç¨‹å¼ç¢¼å„²å­˜ç‚º `translate_file_app_en_to_zh_chunking.py`ã€‚
# 3. åœ¨çµ‚ç«¯æ©Ÿä¸­åŸ·è¡Œï¼š streamlit run translate_file_app_en_to_zh_chunking.py
# ---

# --- API é‡‘é‘°è¨­å®š (ä¿æŒä¸è®Š) ---
api_key = os.getenv("GOOGLE_API_KEY")
# ...(çœç•¥èˆ‡ä¹‹å‰ç‰ˆæœ¬ç›¸åŒçš„ API Key æª¢æŸ¥èˆ‡è¨­å®šç¨‹å¼ç¢¼)...
if not api_key: st.error("..."); st.stop()
try: genai.configure(api_key=api_key)
except Exception as e: st.error(f"...: {e}"); st.stop()


# --- å¾æª”æ¡ˆæå–æ–‡å­—çš„å‡½å¼ (ä¿æŒä¸è®Š) ---
def extract_text_from_file(uploaded_file):
    """
    æ ¹æ“šä¸Šå‚³æª”æ¡ˆçš„é¡å‹æå–æ–‡å­—å…§å®¹ã€‚
    æ”¯æ´ .txt, .docx, .pdf æ ¼å¼ã€‚
    (æ­¤å‡½å¼é‚è¼¯èˆ‡ä¸Šä¸€ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
    """
    extracted_text = ""
    # ...(çœç•¥èˆ‡ä¸Šä¸€ç‰ˆæœ¬ç›¸åŒçš„æª”æ¡ˆè®€å–å’ŒéŒ¯èª¤è™•ç†é‚è¼¯)...
    try:
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        if file_extension == ".txt":
            try: extracted_text = uploaded_file.getvalue().decode("utf-8")
            except UnicodeDecodeError:
                st.warning("å˜—è©¦ UTF-8 è§£ç¢¼å¤±æ•—ï¼Œå˜—è©¦ Big5...")
                try: extracted_text = uploaded_file.getvalue().decode("big5", errors='ignore')
                except Exception as e_enc:
                     st.error(f"å˜—è©¦ Big5 è§£ç¢¼ä¹Ÿå¤±æ•—: {e_enc}ã€‚ä½¿ç”¨å¿½ç•¥éŒ¯èª¤çš„ UTF-8ã€‚")
                     extracted_text = uploaded_file.getvalue().decode("utf-8", errors='ignore')
            st.info(f"æˆåŠŸè®€å– .txt æª”æ¡ˆ: {uploaded_file.name}")
        elif file_extension == ".docx":
            document = docx.Document(uploaded_file)
            extracted_text = '\n'.join([para.text for para in document.paragraphs])
            st.info(f"æˆåŠŸè®€å– .docx æª”æ¡ˆ: {uploaded_file.name}")
        elif file_extension == ".pdf":
            try:
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                if pdf_reader.is_encrypted: st.error("éŒ¯èª¤ï¼šPDF æ–‡ä»¶å·²åŠ å¯†ã€‚"); return None
                full_text = [page.extract_text() for i, page in enumerate(pdf_reader.pages) if page.extract_text() or st.warning(f"è®€å– PDF ç¬¬ {i+1} é æ™‚æœªæå–åˆ°æ–‡å­—æˆ–ç™¼ç”ŸéŒ¯èª¤ã€‚", icon="âš ï¸")]
                extracted_text = '\n'.join(filter(None, full_text))
                if not extracted_text.strip(): st.warning(f"ç„¡æ³•å¾ PDF '{uploaded_file.name}' æå–ä»»ä½•æ–‡å­—ã€‚")
                else: st.info(f"æˆåŠŸè®€å– .pdf æª”æ¡ˆ: {uploaded_file.name}")
            except PdfReadError as pdf_err: st.error(f"PyPDF2 è®€å–éŒ¯èª¤: {pdf_err}"); return None
        else: st.error(f"éŒ¯èª¤ï¼šä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹ '{file_extension}'ã€‚"); return None
        return extracted_text.strip()
    except Exception as e: st.error(f"è®€å–æˆ–è§£ææª”æ¡ˆ '{uploaded_file.name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"); return None


# --- æ–°å¢ï¼šæ–‡æœ¬åˆ†å¡Šå‡½å¼ ---
def split_text_into_chunks(text, max_chars=MAX_CHARS_PER_CHUNK):
    """
    å°‡é•·æ–‡æœ¬åˆ†å‰²æˆå¤šå€‹å¡Šï¼Œç›¡é‡æŒ‰æ®µè½åˆ†å‰²ï¼Œä¸¦ç¢ºä¿æ¯å¡Šä¸è¶…éæœ€å¤§å­—ç¬¦æ•¸ã€‚
    """
    chunks = []
    # å…ˆå˜—è©¦æŒ‰æ®µè½åˆ†å‰² (å‡è¨­æ®µè½ç”±å…©å€‹æ›è¡Œç¬¦åˆ†éš”)
    paragraphs = text.split('\n\n')
    current_chunk = ""

    for paragraph in paragraphs:
        # å¦‚æœæ®µè½æœ¬èº«å°±è¶…é•·ï¼Œéœ€è¦å¼·åˆ¶åˆ†å‰² (é€™è£¡ç”¨ç°¡å–®çš„å­—ç¬¦åˆ†å‰²ï¼Œå¯æ”¹é€²ç‚ºæŒ‰å¥å­)
        if len(paragraph) > max_chars:
            # å¼·åˆ¶åˆ†å‰²é•·æ®µè½
            start = 0
            while start < len(paragraph):
                # å˜—è©¦æ‰¾åˆ°ä¸€å€‹åˆç†çš„æ–·é» (ä¾‹å¦‚å¥è™Ÿã€å•è™Ÿå¾Œ) é è¿‘ max_chars
                # ç°¡åŒ–è™•ç†ï¼šç›´æ¥æŒ‰ max_chars åˆ‡å‰²
                end = min(start + max_chars, len(paragraph))
                long_para_chunk = paragraph[start:end]

                # æª¢æŸ¥åŠ ä¸Šé€™éƒ¨åˆ†æ˜¯å¦æœƒè®“ current_chunk è¶…é•·
                if len(current_chunk) + len(long_para_chunk) + 2 <= max_chars: # +2 for '\n\n'
                     if current_chunk:
                         current_chunk += "\n\n" + long_para_chunk
                     else:
                         current_chunk = long_para_chunk
                else:
                    # å¦‚æœ current_chunk å·²ç¶“æœ‰å…§å®¹ï¼Œå…ˆå„²å­˜å®ƒ
                    if current_chunk:
                        chunks.append(current_chunk)
                    # é€™å€‹é•·æ®µè½çš„ç‰‡æ®µè‡ªæˆä¸€å¡Š (æˆ–æˆç‚ºä¸‹ä¸€å¡Šçš„é–‹é ­)
                    current_chunk = long_para_chunk
                    # å¦‚æœé€™å€‹ç‰‡æ®µæœ¬èº«å°±ç­‰æ–¼æœ€å¤§é•·åº¦ï¼Œç«‹åˆ»å­˜æª”é–‹å§‹æ–°çš„
                    if len(current_chunk) >= max_chars:
                        chunks.append(current_chunk)
                        current_chunk = "" # é–‹å§‹æ–°çš„ chunk

                start = end # ç§»å‹•åˆ°ä¸‹ä¸€å€‹åˆ‡å‰²é»
            # é•·æ®µè½è™•ç†å®Œå¾Œï¼Œå¯èƒ½é‚„æœ‰å‰©é¤˜çš„ current_chunk éœ€è¦è™•ç†
            if current_chunk: # å¦‚æœé•·æ®µè½åˆ‡å®Œæœ€å¾Œä¸€éƒ¨åˆ†æˆç‚ºäº†æ–°çš„ current_chunk
                 pass # ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹æ®µè½ï¼Œçœ‹èƒ½å¦åˆä½µ

        # å¦‚æœæ®µè½ä¸è¶…é•·
        else:
            # æª¢æŸ¥åŠ ä¸Šé€™å€‹æ®µè½æ˜¯å¦æœƒè®“ current_chunk è¶…é•·
            if len(current_chunk) + len(paragraph) + 2 <= max_chars: # +2 for '\n\n'
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # ç•¶å‰å¡Šå·²æ»¿ï¼Œå„²å­˜ç•¶å‰å¡Šï¼Œä¸¦ç”¨æ­¤æ®µè½é–‹å§‹æ–°å¡Š
                chunks.append(current_chunk)
                current_chunk = paragraph

    # æ·»åŠ æœ€å¾Œä¸€å€‹å¡Š (å¦‚æœéç©º)
    if current_chunk:
        chunks.append(current_chunk)

    return chunks


# --- ç¿»è­¯å‡½å¼ (ä¿æŒä½¿ç”¨æµå¼å‚³è¼¸) ---
def translate_text(text_to_translate, target_language="ç¹é«”ä¸­æ–‡"):
    """
    ä½¿ç”¨ Gemini æ¨¡å‹å’Œå›ºå®šçš„è©³ç´°æç¤ºè©ï¼Œé€éæµå¼å‚³è¼¸ç¿»è­¯å–®å€‹æ–‡æœ¬å¡Šã€‚
    (æ­¤å‡½å¼é‚è¼¯èˆ‡ä¸Šä¸€ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
    """
    if not text_to_translate: return None # ç©ºå¡Šç›´æ¥è¿”å›

    # --- ä½¿ç”¨ç°¡åŒ–çš„å›ºå®šæç¤ºè© ---
    # æ›¿æ›æ‰åŸæœ¬è©³ç´°çš„å¤šæ­¥é©Ÿæç¤ºè©
    fixed_instruction_prompt = "Please translate the following English text into accurate and natural Traditional Chinese:"
    # fixed_instruction_prompt = """...""".strip() # çœç•¥ï¼Œä½¿ç”¨ä½ ä¹‹å‰æä¾›çš„å®Œæ•´æç¤ºè©
    # --- ä½¿ç”¨ä½ æä¾›çš„å›ºå®šæç¤ºè© ---
    # fixed_instruction_prompt = """
    # Please act as a professional Chinese translator.
    # I will give you a piece of text, and you will actually follow the steps below to produce a professional Chinese translation that satisfies me.
    # 1. Carefully read and fully understand the original text, ensuring thorough comprehension without haste.
    # 2. Carefully think and consider how you would share the content you just read with your imagined audience in Chinese.
    # 3. Start to translate the text by writting down the proposed sharing content you just had with your imagined audience using traditional Chinese characters. Avoid translating word-for-word; aim for a comfortable, natural, and smooth manner of expression.
    # 4. Compare it with the original text to identify any omissions or inaccuracies, then refine as necessary.
    # """.strip()

    # --- çµ„åˆæç¤ºè©å’Œå¾…ç¿»è­¯æ–‡å­— (ä¿æŒä¸è®Š) ---
    full_prompt = f"{fixed_instruction_prompt}\n\n{text_to_translate}"
    
    # é¸æ“‡æ¨¡å‹ (ä¿æŒä¸è®Š)
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    # model = genai.GenerativeModel('gemini-2.0-flash')

    # ç‹€æ…‹æç¤º (ä¿æŒä¸è®Š)
    # st.info("æ­£åœ¨ä½¿ç”¨æµå¼å‚³è¼¸å°‡è‹±æ–‡æ–‡å­—å‚³é€çµ¦æ¨¡å‹é€²è¡Œç¿»è­¯...") # åœ¨è¿´åœˆä¸­é¡¯ç¤ºå¤ªé »ç¹

    try:
        response = model.generate_content(full_prompt, stream=True)
        full_translated_text = "".join(chunk.text for chunk in response if hasattr(chunk, 'text') and chunk.text)
        return full_translated_text.strip()
    except Exception as e:
        st.error(f"ç¿»è­¯å¡Šæ™‚ API å‘¼å«æˆ–æµå¼è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return f"[[ç¿»è­¯éŒ¯èª¤: {e}]]" # è¿”å›éŒ¯èª¤æ¨™è¨˜è€Œé Noneï¼Œä»¥ä¾¿è¿½è¹¤


# --- å¾æ–‡å­—å»ºç«‹ Docx æª”æ¡ˆå‡½å¼ (ä¿æŒä¸è®Š) ---
def create_docx_from_text(text_content, base_filename):
    """
    å°‡æ–‡å­—å…§å®¹å„²å­˜ç‚º .docx æª”æ¡ˆçš„ BytesIO ç‰©ä»¶ã€‚
    (æ­¤å‡½å¼é‚è¼¯èˆ‡ä¸Šä¸€ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
    """
    try:
        document = Document()
        # æŒ‰æ®µè½æ·»åŠ ï¼Œä¿ç•™æ›è¡Œ
        for paragraph in text_content.split('\n'):
            document.add_paragraph(paragraph)
        # document.add_paragraph(text_content) # åŸæœ¬çš„æ–¹å¼
        docx_buffer = io.BytesIO()
        document.save(docx_buffer)
        docx_buffer.seek(0)
        docx_filename = f"{base_filename}_translated_chunked.docx" # ä¿®æ”¹æª”åå€åˆ¥
        return {'name': docx_filename, 'data': docx_buffer}, None
    except Exception as e:
        error_message = f"å»ºç«‹ Docx æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        st.error(error_message)
        return {'name': None, 'data': None}, error_message


# --- Streamlit æ‡‰ç”¨ç¨‹å¼ä»‹é¢ (ç•¥ä½œèª¿æ•´) ---
st.set_page_config(page_title="æ–‡ä»¶åˆ†å¡Šç¿»è­¯+ä¸‹è¼‰ (è‹±->ç¹ä¸­)", layout="wide")
st.title("ğŸ“ Gemini æ–‡ä»¶åˆ†å¡Šç¿»è­¯ (è‹±æ–‡ â” ç¹é«”ä¸­æ–‡) ä¸¦ä¸‹è¼‰ Docx") # ä¿®æ”¹
st.caption(f"ä¸Šå‚³ .txt, .docx, æˆ– .pdf è‹±æ–‡æ–‡ä»¶ï¼Œå°‡æ–‡æœ¬åˆ†å¡Š (æ¯å¡Šç´„ {MAX_CHARS_PER_CHUNK} å­—ç¬¦) ç¿»è­¯å¾Œåˆä½µä¸¦æä¾› Docx ä¸‹è¼‰") # ä¿®æ”¹

# ...(çœç•¥èˆ‡ä¹‹å‰ç‰ˆæœ¬ç›¸åŒçš„ col1, col2, uploaded_file, base_filename, translate_button å®šç¾©)...
col1, col2 = st.columns(2)
uploaded_file = None
base_filename = None
with col1:
    st.subheader("æ­¥é©Ÿ 1: ä¸Šå‚³è‹±æ–‡æ–‡ä»¶")
    uploaded_file = st.file_uploader(
        "é¸æ“‡è¦ç¿»è­¯çš„è‹±æ–‡æ–‡ä»¶ (.txt, .docx, .pdf)", type=['txt', 'docx', 'pdf'], key="file_uploader"
    )
    if uploaded_file is not None:
        st.markdown(f"**å·²ä¸Šå‚³æª”æ¡ˆ:** `{uploaded_file.name}` (`{uploaded_file.type}`)")
        base_filename = os.path.splitext(uploaded_file.name)[0]
    translate_button = st.button("é–‹å§‹åˆ†å¡Šç¿»è­¯æˆç¹é«”ä¸­æ–‡", key="translate_btn", disabled=uploaded_file is None) # ä¿®æ”¹æŒ‰éˆ•æ–‡å­—

with col2:
    st.subheader("ç¿»è­¯çµæœ (ç¹é«”ä¸­æ–‡)")
    result_placeholder = st.empty()
    result_placeholder.text_area(
        label="ç¿»è­¯çµæœé è¦½å€åŸŸ", value="ç¿»è­¯çµæœå°‡æœƒé¡¯ç¤ºåœ¨é€™è£¡...", height=400,
        key="result_text_area", disabled=True, label_visibility="collapsed"
    )
    download_placeholder = st.empty()
    # æ–°å¢é€²åº¦æ¢ä½”ä½ç¬¦
    progress_placeholder = st.empty()


# --- åŸ·è¡Œæ–‡ä»¶è®€å–ã€åˆ†å¡Šã€ç¿»è­¯ã€åˆä½µ (ä¸»è¦ä¿®æ”¹éƒ¨åˆ†) ---
if translate_button:
    if uploaded_file is None:
        st.warning("è«‹å…ˆä¸Šå‚³ä¸€å€‹æ–‡ä»¶ã€‚")
    else:
        download_placeholder.empty() # æ¸…ç©ºèˆŠä¸‹è¼‰æŒ‰éˆ•
        progress_placeholder.empty() # æ¸…ç©ºèˆŠé€²åº¦æ¢
        result_placeholder.text_area(
            label="ç¿»è­¯çµæœé è¦½å€åŸŸ", value="è™•ç†ä¸­...", height=400,
            key="result_text_area_processing", disabled=True, label_visibility="collapsed"
        )

        # æ­¥é©Ÿ 1: è®€å–æ–‡ä»¶
        with st.spinner(f"æ­£åœ¨è®€å–æª”æ¡ˆ '{uploaded_file.name}'..."):
            extracted_text = extract_text_from_file(uploaded_file)

        if extracted_text is not None and extracted_text.strip():
            st.success("æˆåŠŸå¾æ–‡ä»¶ä¸­æå–è‹±æ–‡æ–‡å­—ï¼")

            # æ­¥é©Ÿ 2: å°‡æå–çš„æ–‡å­—åˆ†å¡Š
            with st.spinner("æ­£åœ¨å°‡æ–‡æœ¬åˆ†å‰²æˆè™•ç†å¡Š..."):
                text_chunks = split_text_into_chunks(extracted_text, MAX_CHARS_PER_CHUNK)
                total_chunks = len(text_chunks)
                if total_chunks == 0:
                    st.warning("æœªèƒ½å°‡æ–‡æœ¬æœ‰æ•ˆåˆ†å‰²æˆå¡Šã€‚")
                    st.stop()
                st.info(f"æ–‡æœ¬å·²åˆ†å‰²æˆ {total_chunks} å€‹å¡Šé€²è¡Œç¿»è­¯ã€‚")

            # æ­¥é©Ÿ 3: é€å¡Šç¿»è­¯ä¸¦åˆä½µçµæœ
            translated_chunks = []
            errors_occurred = False
            progress_bar = progress_placeholder.progress(0) # åˆå§‹åŒ–é€²åº¦æ¢
            status_text = progress_placeholder.text(f"æ­£åœ¨ç¿»è­¯å¡Š 1 / {total_chunks}...")

            fixed_target_language = "ç¹é«”ä¸­æ–‡"

            for i, chunk in enumerate(text_chunks):
                chunk_num = i + 1
                status_text.text(f"æ­£åœ¨ç¿»è­¯å¡Š {chunk_num} / {total_chunks}...")
                # (å¯é¸) é¡¯ç¤ºæ­£åœ¨è™•ç†çš„å¡Š (ç”¨æ–¼é™¤éŒ¯)
                # with st.expander(f"æŸ¥çœ‹å¡Š {chunk_num} åŸæ–‡ (éƒ¨åˆ†)"):
                #    st.text(chunk[:200] + "...")

                # å‘¼å«ç¿»è­¯å‡½å¼ (å…§éƒ¨ä½¿ç”¨æµå¼)
                translated_chunk = translate_text(chunk, fixed_target_language)

                if translated_chunk and "[[ç¿»è­¯éŒ¯èª¤:" not in translated_chunk:
                    translated_chunks.append(translated_chunk)
                else:
                    errors_occurred = True
                    translated_chunks.append(f"\n--- å¡Š {chunk_num} ç¿»è­¯å¤±æ•— ---\n{translated_chunk or 'æœªçŸ¥éŒ¯èª¤'}\n---") # ä¿ç•™éŒ¯èª¤è¨Šæ¯
                    st.error(f"ç¿»è­¯å¡Š {chunk_num} æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚")

                # æ›´æ–°é€²åº¦æ¢
                progress_bar.progress(chunk_num / total_chunks)

                # åœ¨å…©æ¬¡ API å‘¼å«ä¹‹é–“åŠ å…¥å»¶é²
                if chunk_num < total_chunks:
                    time.sleep(API_CALL_DELAY)

            status_text.text("åˆä½µç¿»è­¯çµæœ...")

            # æ­¥é©Ÿ 4: åˆä½µç¿»è­¯çµæœ
            # ä½¿ç”¨æ›è¡Œç¬¦é€£æ¥ï¼Œå¦‚æœåŸå§‹åˆ†å‰²æ˜¯æŒ‰æ®µè½ï¼Œé€™æœƒæ¯”è¼ƒè‡ªç„¶
            final_translated_text = "\n\n".join(translated_chunks)

            # æ­¥é©Ÿ 5: é¡¯ç¤ºçµæœå’Œæä¾›ä¸‹è¼‰
            result_placeholder.text_area(
                label="ç¿»è­¯çµæœ (ç¹é«”ä¸­æ–‡) - åˆä½µè‡ªå¡Š:", value=final_translated_text, height=400,
                key="result_text_area_updated", disabled=False, label_visibility="visible"
            )

            if errors_occurred:
                st.warning("éƒ¨åˆ†æ–‡æœ¬å¡Šç¿»è­¯å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹éŒ¯èª¤è¨Šæ¯åŠåˆä½µçµæœä¸­çš„æ¨™è¨˜ã€‚")
            else:
                st.success("æ‰€æœ‰æ–‡æœ¬å¡Šç¿»è­¯å®Œæˆï¼")

            # ç”¢ç”Ÿä¸¦æä¾› Docx ä¸‹è¼‰ (ä½¿ç”¨åˆä½µå¾Œçš„æ–‡æœ¬)
            with st.spinner("æ­£åœ¨ç”¢ç”Ÿ Docx æª”æ¡ˆ..."):
                docx_data, docx_error = create_docx_from_text(final_translated_text, base_filename)
            if not docx_error and docx_data['data']:
                download_placeholder.download_button(
                    label=f"ğŸ“¥ ä¸‹è¼‰åˆä½µç¿»è­¯çµæœ ({docx_data['name']})",
                    data=docx_data['data'], file_name=docx_data['name'],
                    mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    key='download_docx'
                )
            else:
                download_placeholder.error("æŠ±æ­‰ï¼Œç”¢ç”Ÿ Docx æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚")

        # ... (è™•ç†æå–æ–‡å­—å¤±æ•—æˆ–ç‚ºç©ºçš„æƒ…æ³ï¼Œä¿æŒä¸è®Š) ...
        elif extracted_text is not None and not extracted_text.strip():
             st.warning("å¾æª”æ¡ˆä¸­æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡å­—å…§å®¹ï¼Œç„¡æ³•é€²è¡Œç¿»è­¯ã€‚")
             result_placeholder.text_area("ç¿»è­¯çµæœ (ç¹é«”ä¸­æ–‡):", value="æœªæå–åˆ°æ–‡å­—ã€‚", height=400, key="result_text_area_no_text", disabled=False, label_visibility="visible")
        else:
             st.error("ç„¡æ³•å¾æ–‡ä»¶ä¸­æå–æ–‡å­—ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼æˆ–æŸ¥çœ‹ä¸Šæ–¹çš„éŒ¯èª¤è¨Šæ¯ã€‚")
             result_placeholder.text_area("ç¿»è­¯çµæœ (ç¹é«”ä¸­æ–‡):", value="æ–‡ä»¶è®€å–æˆ–æ–‡å­—æå–å¤±æ•—ã€‚", height=400, key="result_text_area_extract_fail", disabled=False, label_visibility="visible")